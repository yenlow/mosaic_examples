# This YAML is intended to serve as a reference for running on the MosaicML platform
# The config from `yamls/main/mosaic-bert-base-uncased.yaml` is copied into the `parameters` field below.
# You can copy/modify the contents of this file to run different workloads, following the other
# examples in this directory.
#
# Note that some of the fields in this template haven't been filled in yet.
# Please resolve any `null` fields before launching!
#
# When ready, use `mcli run -f yamls/main/mcloud_run_a100_80gb.yaml` to launch

name: mosaic-bert-base-uncased
image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04

compute:
  cluster: r8z11
  gpus: 8  # Number of GPUs to use

  ## These configurations are optional
  # cluster: TODO # Name of the cluster to use for this run
  # gpu_type: a100_80gb # Type of GPU to use. We use a100_80gb in our experiments


integrations:
- integration_type: git_repo
  git_repo: srijitcn/mosaic_examples
  git_branch: develop # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[bert]
  ssh_clone: false # Should be true if using a private repo

# We are fetching, converting, and training on the 'train_small' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `examples/bert/README.md`
# to convert and host the full 'train' dataset.
command: |
  cd mosaic_examples/examples/benchmarks/bert
  pip install -r requirements.txt
  python src/convert_dataset.py --dataset c4 --data_subset en --out_root ./my-copy-c4 --splits train_small val
  composer main.py /mnt/config/parameters.yaml \
    train_loader.dataset.split=train_small

# Starting `parameters` copied from `yamls/main/mosaic-bert-base-uncased.yaml`.
# Changes to `parameters` will be reflected in `/mnt/config/parameters.yaml`, which
# is the config that `main.py` uses in the above command.
parameters:
  data_local: ./my-copy-c4
  data_remote: # If blank, files must be present in data_local

  max_seq_len: 128
  tokenizer_name: bert-base-uncased
  mlm_probability: 0.15

  # Run Name
  run_name: hf-bert-base-uncased

  # Model
  model:
    name: hf_bert
    use_pretrained: false # Train the model from scratch. Set to true to start from the HF off-the-shelf weights.
    pretrained_model_name: ${tokenizer_name}
    tokenizer_name: ${tokenizer_name}
    # This implementation generally uses the default architecture values for from the Hugging Face BertConfig object
    # These values can be changed here when pretraining from scratch. Note that these should only be used
    # if used_pretained: false, otherwise the model will not be loaded properly
    model_config:
      num_attention_heads: 12 # bert-base default
      num_hidden_layers: 12 # bert-base default
      max_position_embedding: 512
      attention_probs_dropout_prob: 0.1 # bert-base default

  # Dataloaders
  train_loader:
    name: text
    dataset:
      local: ${data_local}
      remote: ${data_remote}
      split: train
      tokenizer_name: ${tokenizer_name}
      max_seq_len: ${max_seq_len}
      shuffle: true
      mlm_probability: ${mlm_probability}
    drop_last: true
    num_workers: 8

  eval_loader:
    name: text
    dataset:
      local: ${data_local}
      remote: ${data_remote}
      split: val
      tokenizer_name: ${tokenizer_name}
      max_seq_len: ${max_seq_len}
      shuffle: false
      mlm_probability: 0.15 # We always evaluate at 15% masking for consistent comparison
    drop_last: false
    num_workers: 8

  # Optimization
  scheduler:
    name: linear_decay_with_warmup
    t_warmup: 0.06dur # Warmup to the full LR for 6% of the training duration
    alpha_f: 0.02 # Linearly decay to 0.02x the full LR by the end of the training duration

  optimizer:
    name: decoupled_adamw
    lr: 5.0e-4 # Peak learning rate
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5 # Amount of weight decay regularization

  max_duration: 286720000sp # Subsample the training data for ~275M samples
  eval_interval: 2000ba
  global_train_batch_size: 4096

  # System
  seed: 17
  device_eval_batch_size: 128
  device_train_microbatch_size: 128
  # device_train_microbatch_size: auto
  precision: amp_bf16

  # Logging
  progress_bar: false
  log_to_console: true
  console_log_interval: 1ba


  callbacks:
    speed_monitor:
      window_size: 500
    lr_monitor: {}

  # (Optional) W&B logging
  # loggers:
  #   wandb:
  #     project:      # Fill this in
  #     entity:      # Fill this in

  loggers:
    mlflow:
      tracking_uri: databricks
      experiment_name: mlflow_experiments/bert_pretraining

  #(Optional) Checkpoint to local filesystem or remote object store
  save_interval: 3500ba
  save_num_checkpoints_to_keep: 1  # Important, this cleans up checkpoints saved to DISK
  save_folder: "dbfs:/databricks/mlflow-tracking/{mlflow_experiment_id}/{mlflow_run_id}/artifacts/"